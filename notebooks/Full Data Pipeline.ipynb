{"cells":[{"cell_type":"markdown","source":["# Full Data Pipeline\n## Team 8\n\nData pipeline includes the following steps:\n1. Data Transformation (Joining Weather and Flight dataset)\n  - Reduce the size of weather dataset by keeping only the rows relevant to airports on recordes\n  - Use external dataset to get weather station ID (IATA code)\n  - Aggregate flight data with weather data\n2. Feature engineering\n3. Fill missing values and split the data into tran/test set for ML pipeline and save them into blob storage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c2edc36-8cc7-4d64-8083-c8be0544c3b7"}}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sn\nfrom pyspark.sql.functions import isnull, when, count\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import functions as f\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\n\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import rank, col, monotonically_increasing_id\nimport pyspark\nimport time\nfrom pyspark.ml.feature import Imputer"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94df3ac5-4301-48ee-a9db-0ff87692e0c0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["blob_container = \"w261team8rocks\" # The name of your container created in https://portal.azure.com\nstorage_account = \"dataguru\" # The name of your Storage account created in https://portal.azure.com\nsecret_scope = \"w261-team8\" # The name of the scope created in your local computer using the Databricks CLI\nsecret_key = \"cloudblob\" # The name of the secret key created in your local computer using the Databricks CLI \nblob_url = f\"wasbs://w261team8rocks@dataguru.blob.core.windows.net\"\nmount_path = \"/mnt/mids-w261\"\n\n# SAS Token\nspark.conf.set(\n  f\"fs.azure.sas.w261team8rocks.dataguru.blob.core.windows.net\",\n  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b628413c-4ea6-4e66-90be-6f9be6b9a513"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def data_transformation():\n  '''\n  This function will transformed the given dataset and save the ouput to the blob storage.\n  '''\n  \n  # Load station table\n  df_stations = spark.read.parquet(\"/mnt/mids-w261/datasets_final_project/stations_data/*\")\n  # Zero distance to neighbor means the station is the same as its neighbor.\n  df_stations = df_stations.filter(col(\"distance_to_neighbor\") == 0)\n  print(f\"Stations original n = {df_stations.count()}\")\n  df_stations.createOrReplaceTempView(\"stations\")\n\n  # Load external data to map weather station to IATA codes:\n  adf = pd.read_csv('https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat', header=None)\n  col_names = {\n    0: 'AirportID', 1: 'Name', 2: 'City', 3: 'Country', 4: 'IATA', 5: 'ICAO', 6: 'Latitude', \n    7: 'Longitude', 8: 'Altitude', 9: 'Timezone', 10: 'DST', 11: 'TZ_Timezone', 12: 'Type', 13: 'Source'\n  }\n  adf.rename(columns=col_names, inplace=True)\n  df_airport = spark.createDataFrame(adf)\n  df_airport.createOrReplaceTempView(\"airports\")\n\n  # Join station data with external IATA data\n  query_station_airport = \"\"\"\n  SELECT * \n  FROM \n  (SELECT * FROM stations) AS s \n  LEFT JOIN \n  (SELECT ICAO, IATA, Country, Timezone, DST, TZ_Timezone, Altitude FROM airports) AS a\n  ON s.neighbor_call = a.ICAO\n  \"\"\"\n  stations_with_iata = spark.sql(query_station_airport)\n  print(f\"Stations joined n = {stations_with_iata.count()}\")\n\n  # Write final stations dataset to parquet:\n  stations_with_iata.write.mode(\"overwrite\").parquet(f\"{blob_url}/stations_with_iata\")\n\n  # Load flight data and get unique airports\n  df_airlines = spark.read.parquet(\"/mnt/mids-w261/datasets_final_project/parquet_airlines_data/*\")\n  origin_airports = df_airlines.select(\"ORIGIN\").distinct().collect()\n  dest_airports = df_airlines.select(\"DEST\").distinct().collect()\n  all_airports = set([o[\"ORIGIN\"] for o in origin_airports] + [d[\"DEST\"] for d in dest_airports])\n  unique_airports = spark.createDataFrame([[a] for a in sorted(all_airports)], [\"AIRPORT\"])\n  unique_airports.write.mode(\"overwrite\").parquet(f\"{blob_url}/unique_airports\")\n\n  # Filter stations to only the airports from the full flights dataset:\n  airports = {r[\"AIRPORT\"] for r in unique_airports.select(\"AIRPORT\").distinct().collect()}\n  print(f\"Airports in flights dataset n = {len(airports)}\")\n  stations_with_iata = stations_with_iata.filter(stations_with_iata.IATA.isin(airports))\n  print(f\"Airports found in joined stations n = {stations_with_iata.count()}\")\n  airports_in_joined = {r[\"IATA\"] for r in stations_with_iata.select(\"IATA\").distinct().collect()}\n  airports_not_found = airports - airports_in_joined\n  print(f\"Airports not found: {', '.join(sorted(airports_not_found))}\")\n  display(stations_with_iata)\n\n\n  # Look at the external data for the missing airports:\n  missing_icao = set(adf.loc[(adf[\"IATA\"].isin(airports_not_found)), \"ICAO\"])\n  print('missing airports')\n  display(adf.loc[(adf[\"IATA\"].isin(airports_not_found))])\n\n  # Look at the counts of flights from airports not found:\n  flights_from_bad_airports = df_airlines.filter(df_airlines[\"ORIGIN\"].isin(airports_not_found))\n  print('Check counts of flights from airports not found')\n  display(flights_from_bad_airports.groupby(\"ORIGIN\").count())\n\n\n  # Get the unique relevant station IDs from the final station table:\n  station_ids = {r[\"station_id\"] for r in stations_with_iata.select(col(\"station_id\")).distinct().collect()}\n\n\n  # Load weather table:\n  df_weather = spark.read.parquet(\"/mnt/mids-w261/datasets_final_project/weather_data/*\")\n\n  original_weather_count = df_weather.count()\n  print(f\"Original weather n = {original_weather_count}\")\n\n\n  # Filter the weather table to only the relevant stations:\n  df_weather_filtered = df_weather.filter(df_weather.STATION.isin(station_ids))\n  filtered_weather_count = df_weather_filtered.count()\n  print(f\"Filtered weather n = {filtered_weather_count}\")\n  print(f\"Weather data size reduced by {(1-(filtered_weather_count/original_weather_count))*100:.0f}%\")\n\n  # Round weather data to nearest hour to merge with flights, and then shift by 2 hours.\n  # First shift by -1 minutes (so that rows exactly on the hour aren't shifted 3 hours),\n  # then shift by 3 hours so that each row is at least 2 hours from its original timestamp.\n\n  weather_original_columns = df_weather_filtered.columns\n\n  # Shift by 2 hours:\n  df_weather_filtered = df_weather_filtered.withColumn(\n    \"shifted_timestamp\", df_weather_filtered[\"DATE\"] + expr(\"INTERVAL -1 MINUTES\")\n  )\n  df_weather_filtered = df_weather_filtered.withColumn(\n    \"shifted_timestamp\", df_weather_filtered[\"shifted_timestamp\"] + expr(\"INTERVAL 3 HOURS\")\n  )\n\n  # Truncate hour (i.e. set minutes and everything after to 0):\n  df_weather_filtered = df_weather_filtered.withColumn(\n    \"final_timestamp\", date_trunc(\"hour\", df_weather_filtered.shifted_timestamp)\n  )\n\n  # Rearrange columns:\n  df_weather_filtered = df_weather_filtered.select(\n    weather_original_columns[:2] + [\"final_timestamp\"] + weather_original_columns[2:]\n  )\n\n\n  # There will be lots of duplicates by station ID and final_timestamp. \n  # Drop duplicates ordered by station ID and original datestamp, to keep\n  # the observation closest to the final_timestamp.\n  # This method for dropping duplicates adapted from:\n  # https://stackoverflow.com/a/54738843/6286540\n\n\n  window = Window.partitionBy(\"STATION\", \"final_timestamp\").orderBy(\"DATE\", \"tiebreak\")\n  df_weather_deduped = df_weather_filtered\\\n    .withColumn(\"tiebreak\", monotonically_increasing_id())\\\n    .withColumn(\"rank\", rank().over(window))\\\n    .filter(col(\"rank\") == 1).drop(\"rank\", \"tiebreak\")\n\n  print('Weather data after dropping duplicates')\n  display(df_weather_deduped)\n\n  # Merge relevant weather data with station info:\n\n  weather_keep_columns = [\n    \"STATION\", \"DATE\", \"final_timestamp\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"NAME\", \n    \"WND\", \"CIG\", \"VIS\", \"TMP\", \"DEW\", \"SLP\", \n  ]\n\n  station_keep_cols = [\n    \"station_id\", \"neighbor_state\", \"ICAO\", \"IATA\", \"Country\", \"Timezone\", \"DST\", \"TZ_Timezone\", \"Altitude\"\n  ]\n\n  stations_with_iata.select(station_keep_cols).createOrReplaceTempView(\"stations\")\n  df_weather_deduped.select(weather_keep_columns).createOrReplaceTempView(\"weather\")\n\n  query_weather_stations = f\"\"\"\n  SELECT * \n  FROM \n  (SELECT {', '.join(weather_keep_columns)} FROM weather) AS w\n  LEFT JOIN \n  (SELECT {', '.join(station_keep_cols)} FROM stations) AS s\n  ON w.STATION = s.station_id\n  \"\"\"\n\n  joined_weather_stations = spark.sql(query_weather_stations)\n  print('Joined weather and station data')\n  display(joined_weather_stations)  \n\n\n  # Drop irrelevant flight table columns:\n  # (e.g. dropped \"Gate Return Information at Origin Airport (Data starts 10/2008)\" and \"Diverted Airport Information (Data starts 10/2008)\" sections)\n\n  flights_keep_columns = [\n    'YEAR', 'QUARTER', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE',\n    'OP_UNIQUE_CARRIER', 'OP_CARRIER_AIRLINE_ID', 'OP_CARRIER', 'TAIL_NUM', 'OP_CARRIER_FL_NUM',\n    'ORIGIN_AIRPORT_ID', 'ORIGIN_AIRPORT_SEQ_ID', 'ORIGIN_CITY_MARKET_ID', 'ORIGIN', 'ORIGIN_CITY_NAME',\n    'ORIGIN_STATE_ABR', 'ORIGIN_STATE_FIPS', 'ORIGIN_STATE_NM', 'ORIGIN_WAC', \n    'DEST_AIRPORT_ID', 'DEST_AIRPORT_SEQ_ID', 'DEST_CITY_MARKET_ID', 'DEST', 'DEST_CITY_NAME', \n    'DEST_STATE_ABR', 'DEST_STATE_FIPS', 'DEST_STATE_NM', 'DEST_WAC',\n    'CRS_DEP_TIME', 'DEP_TIME', 'DEP_DELAY', 'DEP_DELAY_NEW', 'DEP_DEL15', 'DEP_DELAY_GROUP', 'DEP_TIME_BLK', \n    'TAXI_OUT', 'WHEELS_OFF', 'WHEELS_ON', 'TAXI_IN', \n    'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY', 'ARR_DELAY_NEW', 'ARR_DEL15', 'ARR_DELAY_GROUP', 'ARR_TIME_BLK',\n    'CANCELLED', 'CANCELLATION_CODE', 'DIVERTED', 'CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME', 'AIR_TIME', \n    'FLIGHTS', 'DISTANCE', 'DISTANCE_GROUP', \n    'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY'\n  ]\n\n  flights = df_airlines.select(*flights_keep_columns)\n\n\n\n  # Additional clean up to drop flight records:\n\n  # Assumption 1: Remove cancelled flights\n  \"\"\"When DEP_DEL15.isNull(), these are cancelled flights which are approximately 1.5% of total flights. Our Phase I results indicated that flights are cancelled due to the following reasons (\"A\": \"Carrier\", \"B\": \"Weather\", \"C\": \"National Air System\", \"D\": \"Security\"). We can safely take out cancelled flights (null value) from the delayed flights (\"DEP_DEL15\") since they are not relevant and minimal.\n  \"\"\"\n  flights = flights.where(col(\"CANCELLED\") != 1)\n\n\n  # Assumption 2: Remove diverted flights\n  \"\"\"A flight diversion is when an aircraft is unable to arrive at its final destination. Such as Aircraft emergency; Passenger emergency; Mechanical failure; and Poor weather conditions. We decided to remove this since it's not relevant to our analysis.\"\"\"\n  flights = flights.where(col(\"DIVERTED\") != 1)\n\n\n  # Drop any duplicate rows in full dataset:\n  flights = flights.dropDuplicates()\n\n\n  # There are an additional 4725 rows where for some reason the departure delay columns are null.\n  # On inspection, in all these rows the scheduled CRS_DEP_TIME is equal to the DEP_TIME, meaning the delay is 0 minutes.\n  # Hence we fill these columns with 0:\n  flights = flights.fillna(value=0, subset=[\"DEP_DELAY\", \"DEP_DELAY_NEW\", \"DEP_DEL15\", \"DEP_DELAY_GROUP\"])\n\n  # Add origin and destination timezone columns to flights data:\n\n  stations_with_iata.select([\"IATA\", \"Timezone\", \"TZ_Timezone\"]).createOrReplaceTempView(\"timezones\")\n\n  # Origin:\n  flights.createOrReplaceTempView(\"flights\")\n  query_flights_timezone = f\"\"\"\n  SELECT * \n  FROM \n  (SELECT * FROM flights) AS f\n  LEFT JOIN \n  (SELECT IATA AS ORIGIN_IATA, Timezone AS ORIGIN_Timezone, TZ_Timezone AS ORIGIN_TZ FROM timezones) AS tz\n  ON f.ORIGIN = tz.ORIGIN_IATA\n  \"\"\"\n  flights = spark.sql(query_flights_timezone)\n\n  # Destination:\n  flights.createOrReplaceTempView(\"flights\")\n  query_flights_timezone = f\"\"\"\n  SELECT * \n  FROM \n  (SELECT * FROM flights) AS f\n  LEFT JOIN \n  (SELECT IATA AS DEST_IATA, Timezone AS DEST_Timezone, TZ_Timezone AS DEST_TZ FROM timezones) AS tz\n  ON f.DEST = tz.DEST_IATA\n  \"\"\"\n  flights = spark.sql(query_flights_timezone)\n  \n  # Convert flight Departure Times to UTC and round to nearest hour:\n\n  # Convert departure time integers to zero-padded strings, e.g. 607 -> 0000607:\n  # Modification: Use scheduled departure time instead\n  flights = flights.withColumn(\"PADDED_DEP_TIME\", format_string(\"0000%d\", \"CRS_DEP_TIME\"))\n  # Shorten the strings to the final 4 chars, e.g. 0000607 -> 0607:\n  flights = flights.withColumn(\"FORMATTED_DEP_TIME\", substring(\"PADDED_DEP_TIME\", -4,4))\n  # Concatenate string columns for departure date and time:\n  flights = flights.withColumn(\"DEPT_DT_STR\", concat_ws(\" \", flights.FL_DATE, flights.FORMATTED_DEP_TIME))\n  # Convert string datetime to timestamp:\n  flights = flights.withColumn(\"DEPT_DT\", to_timestamp(flights.DEPT_DT_STR, \"yyyy-MM-dd HHmm\"))\n  # Use datetime and timezone to convert dates to UTC:\n  flights = flights.withColumn(\"DEPT_UTC\", to_utc_timestamp(flights.DEPT_DT, flights.ORIGIN_TZ))\n  # Remove minutes and round datetimes *down* to nearest hour. It is necessary to round\n  # down so that we don't join with weather data from less than 2 hours before:\n  flights = flights.withColumn(\"DEPT_UTC_HOUR\", date_trunc(\"HOUR\", flights.DEPT_UTC))\n\n  # Calculate arrival time in UTC using departure time and elapsed time:\n  # Modification: Use scheduled elapsed time\n  flights = flights.withColumn(\"ARR_UTC\", col(\"DEPT_UTC\") + (col(\"CRS_ELAPSED_TIME\") * expr(\"Interval 1 Minutes\")))  \n  \n  \n   # Join flights and weather data (origin and destination) on airport and time:\n\n  flights.createOrReplaceTempView(\"flights\")\n  origin_weather_statioins = joined_weather_stations.select('final_timestamp','IATA','WND','CIG','VIS','TMP','DEW','SLP')\n  origin_weather_statioins = origin_weather_statioins.withColumnRenamed('IATA', 'weather_ORIGIN_IATA')\\\n                               .withColumnRenamed('final_timestamp', 'DEPT_UTC_HOUR_ORIGIN')\\\n                               .withColumnRenamed('SLP', 'SLP_ORIGIN')\\\n                               .withColumnRenamed('WND', 'WND_ORIGIN')\\\n                               .withColumnRenamed('CIG', 'CIG_ORIGIN')\\\n                               .withColumnRenamed('VIS', 'VIS_ORIGIN')\\\n                               .withColumnRenamed('TMP', 'TMP_ORIGIN')\\\n                               .withColumnRenamed('DEW', 'DEW_ORIGIN')\n  destination_weather_statioins = joined_weather_stations.select('final_timestamp','IATA','WND','CIG','VIS','TMP','DEW','SLP')\n  destination_weather_statioins = destination_weather_statioins.withColumnRenamed('IATA', 'weather_DEST_IATA')\\\n                               .withColumnRenamed('final_timestamp', 'DEPT_UTC_HOUR_DEST')\\\n                               .withColumnRenamed('SLP', 'SLP_DEST')\\\n                               .withColumnRenamed('WND', 'WND_DEST')\\\n                               .withColumnRenamed('CIG', 'CIG_DEST')\\\n                               .withColumnRenamed('VIS', 'VIS_DEST')\\\n                               .withColumnRenamed('TMP', 'TMP_DEST')\\\n                               .withColumnRenamed('DEW', 'DEW_DEST')\n\n  origin_weather_statioins.createOrReplaceTempView(\"ORIGIN_weather\")\n  destination_weather_statioins.createOrReplaceTempView(\"DEST_weather\")  \n\n  final_df = spark.sql('''\n  select *\n  from flights as f\n  inner join ORIGIN_weather as ow on f.ORIGIN = ow.weather_ORIGIN_IATA AND f.DEPT_UTC_HOUR = ow.DEPT_UTC_HOUR_ORIGIN\n  inner join DEST_weather as dw on f.DEST = dw.weather_DEST_IATA AND f.DEPT_UTC_HOUR = dw.DEPT_UTC_HOUR_DEST\n\n  ''')\n\n\n  # Get statistics of the final dataset\n  final_n = final_df.count()\n  flights_n = flights.count()\n  print(f\"Final dataset has {final_n:,} rows ({flights_n-final_n:,} dropped from original flights dataset)\")\n\n\n  # Split weather columns into individual data columns and deal with null-coded values:\n\n  # Wind:\n  final_df = final_df.withColumn(\"WND_dir_ORIGIN\", split(final_df[\"WND_ORIGIN\"], \",\").getItem(0)) \\\n                     .withColumn(\"WND_dir_qlty_ORIGIN\", split(final_df[\"WND_ORIGIN\"], \",\").getItem(1)) \\\n                     .withColumn(\"WND_type_ORIGIN\", split(final_df[\"WND_ORIGIN\"], \",\").getItem(2)) \\\n                     .withColumn(\"WND_spd_ORIGIN\", split(final_df[\"WND_ORIGIN\"], \",\").getItem(3)) \\\n                     .withColumn(\"WND_spd_qlty_ORIGIN\", split(final_df[\"WND_ORIGIN\"], \",\").getItem(4))\\\n                    .withColumn(\"WND_dir_DEST\", split(final_df[\"WND_DEST\"], \",\").getItem(0)) \\\n                    .withColumn(\"WND_dir_qlty_DEST\", split(final_df[\"WND_DEST\"], \",\").getItem(1)) \\\n                    .withColumn(\"WND_type_DEST\", split(final_df[\"WND_DEST\"], \",\").getItem(2)) \\\n                    .withColumn(\"WND_spd_DEST\", split(final_df[\"WND_DEST\"], \",\").getItem(3)) \\\n                    .withColumn(\"WND_spd_qlty_DEST\", split(final_df[\"WND_DEST\"], \",\").getItem(4))\n\n  # CIG:\n  final_df = final_df.withColumn(\"CIG_ceil_height_ORIGIN\", split(final_df[\"CIG_ORIGIN\"], \",\").getItem(0)) \\\n                     .withColumn(\"CIG_ceil_qlty_ORIGIN\", split(final_df[\"CIG_ORIGIN\"], \",\").getItem(1)) \\\n                     .withColumn(\"CIG_ceil_det_code_ORIGIN\", split(final_df[\"CIG_ORIGIN\"], \",\").getItem(2)) \\\n                     .withColumn(\"CIG_cavok_code_ORIGIN\", split(final_df[\"CIG_ORIGIN\"], \",\").getItem(3))\\\n                  .withColumn(\"CIG_ceil_height_DEST\", split(final_df[\"CIG_DEST\"], \",\").getItem(0)) \\\n                  .withColumn(\"CIG_ceil_qlty_DEST\", split(final_df[\"CIG_DEST\"], \",\").getItem(1)) \\\n                  .withColumn(\"CIG_ceil_det_code_DEST\", split(final_df[\"CIG_DEST\"], \",\").getItem(2)) \\\n                  .withColumn(\"CIG_cavok_code_DEST\", split(final_df[\"CIG_DEST\"], \",\").getItem(3))\n\n  # Visibility:\n  final_df = final_df.withColumn(\"VIS_dim_ORIGIN\", split(final_df[\"VIS_ORIGIN\"], \",\").getItem(0)) \\\n                     .withColumn(\"VIS_dim_qlty_ORIGIN\", split(final_df[\"VIS_ORIGIN\"], \",\").getItem(1)) \\\n                     .withColumn(\"VIS_var_ORIGIN\", split(final_df[\"VIS_ORIGIN\"], \",\").getItem(2)) \\\n                     .withColumn(\"VIS_var_qlty_ORIGIN\", split(final_df[\"VIS_ORIGIN\"], \",\").getItem(3))\\\n                  .withColumn(\"VIS_dim_DEST\", split(final_df[\"VIS_DEST\"], \",\").getItem(0)) \\\n                  .withColumn(\"VIS_dim_qlty_DEST\", split(final_df[\"VIS_DEST\"], \",\").getItem(1)) \\\n                  .withColumn(\"VIS_var_DEST\", split(final_df[\"VIS_DEST\"], \",\").getItem(2)) \\\n                  .withColumn(\"VIS_var_qlty_DEST\", split(final_df[\"VIS_DEST\"], \",\").getItem(3))\n\n  # Temperature:\n  final_df = final_df.withColumn(\"TMP_air_ORIGIN\", split(final_df[\"TMP_ORIGIN\"], \",\").getItem(0)) \\\n                     .withColumn(\"TMP_air_qlty_ORIGIN\", split(final_df[\"TMP_ORIGIN\"], \",\").getItem(1))\\\n                  .withColumn(\"TMP_air_DEST\", split(final_df[\"TMP_DEST\"], \",\").getItem(0)) \\\n                  .withColumn(\"TMP_air_qlty_DEST\", split(final_df[\"TMP_DEST\"], \",\").getItem(1))\n\n  # Dew:\n  final_df = final_df.withColumn(\"DEW_point_temp_ORIGIN\", split(final_df[\"DEW_ORIGIN\"], \",\").getItem(0)) \\\n                     .withColumn(\"DEW_point_qlty_ORIGIN\", split(final_df[\"DEW_ORIGIN\"], \",\").getItem(1))\\\n                  .withColumn(\"DEW_point_temp_DEST\", split(final_df[\"DEW_DEST\"], \",\").getItem(0)) \\\n                  .withColumn(\"DEW_point_qlty_DEST\", split(final_df[\"DEW_DEST\"], \",\").getItem(1))\n\n  # Sea-level pressure:\n  final_df = final_df.withColumn(\"SLP_pressure_ORIGIN\", split(final_df[\"SLP_ORIGIN\"], \",\").getItem(0)) \\\n                     .withColumn(\"SLP_pressure_qlty_ORIGIN\", split(final_df[\"SLP_ORIGIN\"], \",\").getItem(1))\\\n                    .withColumn(\"SLP_pressure_DEST\", split(final_df[\"SLP_DEST\"], \",\").getItem(0)) \\\n                    .withColumn(\"SLP_pressure_qlty_DEST\", split(final_df[\"SLP_DEST\"], \",\").getItem(1))  \n\n\n  # Replace null-codes:\n  col_null_codes = {\n    \"WND_dir_ORIGIN\": \"999\",\n    \"WND_type_ORIGIN\": \"9\",\n    \"WND_spd_ORIGIN\": \"9999\",\n    \"CIG_ceil_height_ORIGIN\": \"99999\",\n    \"CIG_ceil_det_code_ORIGIN\": \"9\",\n    \"CIG_cavok_code_ORIGIN\": \"9\",\n    \"VIS_dim_ORIGIN\": \"999999\",\n    \"VIS_var_ORIGIN\": \"9\",\n    \"TMP_air_ORIGIN\": \"+9999\", \n    \"DEW_point_temp_ORIGIN\": \"+9999\",\n    \"SLP_pressure_ORIGIN\": \"99999\",\n\n    \"WND_dir_DEST\": \"999\",\n    \"WND_type_DEST\": \"9\",\n    \"WND_spd_DEST\": \"9999\",\n    \"CIG_ceil_height_DEST\": \"99999\",\n    \"CIG_ceil_det_code_DEST\": \"9\",\n    \"CIG_cavok_code_DEST\": \"9\",\n    \"VIS_dim_DEST\": \"999999\",\n    \"VIS_var_DEST\": \"9\",\n    \"TMP_air_DEST\": \"+9999\", \n    \"DEW_point_temp_DEST\": \"+9999\",\n    \"SLP_pressure_DEST\": \"99999\"\n  }\n  for col_name, null_code in col_null_codes.items():\n    final_df = final_df.replace(null_code, value=None, subset=[col_name])\n\n  # Convert columns types:\n  float_cols = [\"WND_dir_ORIGIN\", \"WND_spd_ORIGIN\", \"CIG_ceil_height_ORIGIN\", \"VIS_dim_ORIGIN\", \"TMP_air_ORIGIN\", \"DEW_point_temp_ORIGIN\", \"SLP_pressure_ORIGIN\",\n               \"WND_dir_DEST\", \"WND_spd_DEST\", \"CIG_ceil_height_DEST\", \"VIS_dim_DEST\", \"TMP_air_DEST\", \"DEW_point_temp_DEST\", \"SLP_pressure_DEST\"]\n  for f_col in float_cols:\n    final_df = final_df.withColumn(f_col, final_df[f_col].cast(FloatType()))  \n\n\n  # Save full final dataset:\n  final_df.write.mode(\"overwrite\").parquet(f\"{blob_url}/team8_full_dataset_V2\")\n\n  return 'Weather, Station and Flight Dataset are successfully joined'  \n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90b93d67-b4da-4383-ad74-49f39680109c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def get_avg_delay(flight_data):\n  '''\n  Flight_data is assumed to have departure time in utc and truncated down to nearest hour\n  Output is a spark dataframe with schema: ORIGIN, 6_hour_before_departure, 2_hour_before_departure, avg_delay\n  \n  Join the original flight data with output spark data frame by ORIGIN, 6_hour_before_departure, 2_hour_before_departure\n  \n  '''\n  \n  transformed_flight_data = flight_data.withColumn('6_hour_before_departure', flight_data['DEPT_UTC_HOUR'] - expr('INTERVAL 6 hours'))\\\n                                       .withColumn('2_hour_before_departure', flight_data['DEPT_UTC_HOUR'] - expr('INTERVAL 2 hours'))\n  transformed_flight_data.createOrReplaceTempView('flight_temp')\n  \n  delay_df = spark.sql('''\n  select f2.ORIGIN, f2.6_hour_before_departure, f2.2_hour_before_departure, avg(f1.DEP_DELAY) as avg_delay\n  from flight_temp as f1\n  inner join flight_temp as f2 on (f1.DEPT_UTC_HOUR between f2.6_hour_before_departure and f2.2_hour_before_departure) and (f1.ORIGIN = f2.ORIGIN)\n  group by 1,2,3\n  order by 1,2,3\n  '''\n  )  \n  \n  return delay_df\n\ndef feature_engineering():\n  \n  '''\n  This function will add new features to the transformed dataset.\n  The output includes full set of features that will be used to build the model\n  '''\n  \n\n  '''\n  Feature 1: average delay\n  This feature needs to be calculated year by year; otherwise it takes very long to run. Splitting by years and concatenating later can improve processing time significantly\n  '''\n  full_dataset = spark.read.parquet(f\"{blob_url}/team8_full_dataset_V2\")\n  for y in [2015,2016,2017,2018,2019]:\n    start_time = time.time()\n    print(f'Year {y}')\n    year_data = full_dataset.filter((full_dataset['DEPT_UTC_HOUR'] >= f'{y - 1}-12-31') & (full_dataset['DEPT_UTC_HOUR'] < f'{y + 1}-1-1')) # include one day from previous year to make sure average delay is calculated correctly for the first day in the year. e.g to calculate average delay for a flight scheduled at 2017-1-1 2:00 am, we need data from previous year   \n    delay_df = get_avg_delay(year_data)\n    delay_df = delay_df.filter(delay_df['6_hour_before_departure'] >= f'{y}-1-1') # remove data that doesn't belong to current year\n    delay_df.write.mode(\"overwrite\").parquet(f\"{blob_url}/avg_delay_{y}_v2\")\n    print('Done')\n    print(\"--- %s seconds ---\" % (time.time() - start_time))\n    \n  \n  # Generate keys to join with transformed dataset\n  full_dataset = full_dataset.withColumn('6_hour_before_departure', full_dataset['DEPT_UTC_HOUR'] - expr('INTERVAL 6 hours'))\\\n                                       .withColumn('2_hour_before_departure', full_dataset['DEPT_UTC_HOUR'] - expr('INTERVAL 2 hours'))\n  \n  # Decompose the transformed dataset by year and join the average delay by year. In this way, runtime can be reduced significantly\n  data_2015 = full_dataset.filter(year(full_dataset['DEPT_UTC_HOUR']) == 2015)\n  data_2016 = full_dataset.filter(year(full_dataset['DEPT_UTC_HOUR']) == 2016)\n  data_2017 = full_dataset.filter(year(full_dataset['DEPT_UTC_HOUR']) == 2017)\n  data_2018 = full_dataset.filter(year(full_dataset['DEPT_UTC_HOUR']) == 2018)\n  data_2019 = full_dataset.filter(year(full_dataset['DEPT_UTC_HOUR']) == 2019)\n\n  # Rename the columns of delay dataframes. Later they will be used as key to join other dataframes and get average delay for both origin and destination\n  delay_2015_ORIGIN = spark.read.parquet(f\"{blob_url}/avg_delay_2015_v2\")\n  delay_2015_ORIGIN = delay_2015_ORIGIN.withColumnRenamed(\"avg_delay\",\"avg_delay_ORIGIN\")\n  delay_2016_ORIGIN = spark.read.parquet(f\"{blob_url}/avg_delay_2016_v2\")\n  delay_2016_ORIGIN = delay_2016_ORIGIN.withColumnRenamed(\"avg_delay\",\"avg_delay_ORIGIN\")\n  delay_2017_ORIGIN = spark.read.parquet(f\"{blob_url}/avg_delay_2017_v2\")\n  delay_2017_ORIGIN = delay_2017_ORIGIN.withColumnRenamed(\"avg_delay\",\"avg_delay_ORIGIN\")\n  delay_2018_ORIGIN = spark.read.parquet(f\"{blob_url}/avg_delay_2018_v2\")\n  delay_2018_ORIGIN = delay_2018_ORIGIN.withColumnRenamed(\"avg_delay\",\"avg_delay_ORIGIN\")\n  delay_2019_ORIGIN = spark.read.parquet(f\"{blob_url}/avg_delay_2019_v2\")\n  delay_2019_ORIGIN = delay_2019_ORIGIN.withColumnRenamed(\"avg_delay\",\"avg_delay_ORIGIN\")\n\n  delay_2015_DEST = spark.read.parquet(f\"{blob_url}/avg_delay_2015_v2\")\n  delay_2015_DEST = delay_2015_DEST.withColumnRenamed(\"avg_delay\",\"avg_delay_DEST\")\\\n                                   .withColumnRenamed(\"ORIGIN\",\"DEST\")\n  delay_2016_DEST = spark.read.parquet(f\"{blob_url}/avg_delay_2016_v2\")\n  delay_2016_DEST = delay_2016_DEST.withColumnRenamed(\"avg_delay\",\"avg_delay_DEST\")\\\n                                   .withColumnRenamed(\"ORIGIN\",\"DEST\")\n  delay_2017_DEST = spark.read.parquet(f\"{blob_url}/avg_delay_2017_v2\")\n  delay_2017_DEST = delay_2017_DEST.withColumnRenamed(\"avg_delay\",\"avg_delay_DEST\")\\\n                                   .withColumnRenamed(\"ORIGIN\",\"DEST\")\n  delay_2018_DEST = spark.read.parquet(f\"{blob_url}/avg_delay_2018_v2\")n\n                                   .withColumnRenamed(\"ORIGIN\",\"DEST\")\n  delay_2019_DEST = spark.read.parquet(f\"{blob_url}/avg_delay_2019_v2\")\n  delay_2019_DEST = delay_2019_DEST.withColumnRenamed(\"avg_delay\",\"avg_delay_DEST\")\\\n                                   .withColumnRenamed(\"ORIGIN\",\"DEST\")    \n  \n  # Join the split dataset with average delay data (by year) on both origin and destination\n  data_2015 = data_2015.join(delay_2015_ORIGIN, \n                             on = ['ORIGIN','2_hour_before_departure','6_hour_before_departure'],\n                             how = 'left')\\\n                       .join(delay_2015_DEST, \n                             on = ['DEST','2_hour_before_departure','6_hour_before_departure'],\n                             how = 'left')\n\n  data_2016 = data_2016.join(delay_2016_ORIGIN, \n                             on = ['ORIGIN','2_hour_before_departure','6_hour_before_departure'],\n                             how = 'left')\\\n                       .join(delay_2016_DEST, \n                             on = ['DEST','2_hour_before_departure','6_hour_before_departure'],\n                             how = 'left')\n\n  data_2017 = data_2017.join(delay_2017_ORIGIN, \n                             on = ['ORIGIN','2_hour_before_departure','6_hour_before_departure'],\n                             how = 'left')\\\n                       .join(delay_2017_DEST, \n                             on = ['DEST','2_hour_before_departure','6_hour_before_departure'],\n                             how = 'left')\n\n  data_2018 = data_2018.join(delay_2018_ORIGIN, \n                             on = ['ORIGIN','2_hour_before_departure','6_hour_before_departure'],\n                             how = 'left')\\\n                       .join(delay_2018_DEST, \n                             on = ['DEST','2_hour_before_departure','6_hour_before_departure'],\n                             how = 'left')\n\n  data_2019 = data_2019.join(delay_2019_ORIGIN, \n                             on = ['ORIGIN','2_hour_before_departure','6_hour_before_departure'],\n                             how = 'left')\\\n                       .join(delay_2019_DEST, \n                             on = ['DEST','2_hour_before_departure','6_hour_before_departure'],\n                             how = 'left')    \n\n  data = data_2015.union(data_2016).union(data_2017).union(data_2018).union(data_2019)\n  \n  #Feature 2: Prior Flight Delay & Potentiail for Delay Indicator\n  '''\n  Feature 2: Prior Flight Delay & Potentiail for Delay Indicator\n  (1) Previous Flight Delay Indicator (Categorical value) - Is delayed set to 1; not delayed set to 0; otherwise null.\n  (2) Potential for Delay Indicator (Categorical value) - Set to null if flight arrives more than 2 hrs before departure, the likelihood for delay is smaller; Set to 1 if flights arrives less than 2 hrs before departure.\n  '''\n\n  # First filter for rows where actual arrival date is greater than departure date \n  data = data.withColumn(\"ARR_UTC\", f.when((data.ARR_UTC < data.DEPT_UTC),(f.from_unixtime(f.unix_timestamp('DEPT_UTC') + (data.ACTUAL_ELAPSED_TIME*60)))).otherwise(data.ARR_UTC))\n\n  # Group by tail number, then sort by actual arrival time\n  tail_group = Window.partitionBy('tail_num').orderBy('ARR_UTC')\n\n  data = data.withColumn('prev_actual_arr_utc', f.lag('ARR_UTC',1, None).over(tail_group))\\ # prior actual arrival time of each flight\n              .withColumn('prev_fl_del', f.lag('DEP_DEL15',1, None).over(tail_group)) # flag for 1 if previous flight is delayed for the same airplane (identified by tail number)\n  \n  data = data.withColumn(\"planned_departure_utc\", col(\"DEPT_UTC\") - (col(\"DEP_DELAY\") * expr(\"Interval 1 Minutes\")))\\\n              .withColumn('inbtwn_fl_hrs', (f.unix_timestamp('planned_departure_utc') - f.unix_timestamp('prev_actual_arr_utc'))/60/60)\\ # Calculate the hours in between prior actual arrival time and planned departure time\n              .withColumn('poten_for_del', expr(\"CASE WHEN inbtwn_fl_hrs > 2 THEN '0'\" + \"ELSE '1' END\")) # Categorize flight gap (>2 hours = 0, < 2 hours = 1) Has the airplane arrived 2 hours before departure? Simplify to 1 if airplane is in the airport less than 2 hours before departure, otherwise 0 if not or null.\n\n  \n  #Feature 3: Holiday Indicator\n  data = data.withColumn('holiday', expr(\"\"\"CASE WHEN FL_DATE in ('2015-12-25', '2016-12-25', '2017-12-25', '2018-12-25', '2019-12-25',\n                                                           '2015-11-26', '2016-11-24', '2017-11-23', '2018-11-22', '2019-11-28', \n                                                           '2015-01-01', '2016-01-01', '2017-01-01', '2018-01-01', '2019-01-01',\n                                                           '2015-07-04', '2016-07-04', '2017-07-04', '2018-07-04', '2019-07-04') THEN 'holiday' \"\"\" + \n                                         \"\"\" WHEN FL_DATE in ('2015-12-23', '2015-12-24', '2015-12-26', '2015-12-27',\n                                                              '2016-12-23', '2016-12-24', '2016-12-26', '2016-12-27',\n                                                              '2017-12-23', '2017-12-24', '2017-12-26', '2017-12-27',\n                                                              '2018-12-23', '2018-12-24', '2018-12-26', '2018-12-27',\n                                                              '2019-12-23', '2019-12-24', '2019-12-26', '2019-12-27',\n                                                              '2015-11-24', '2015-11-25', '2015-11-27', '2015-11-28',\n                                                              '2016-11-22', '2016-11-24', '2016-11-25', '2016-11-26',\n                                                              '2017-11-21', '2017-11-22', '2017-11-24', '2017-11-25',\n                                                              '2018-11-20', '2018-11-21', '2018-11-23', '2018-11-24',\n                                                              '2019-11-26', '2019-11-27', '2019-11-29', '2019-11-30', \n                                                              '2015-01-02', '2015-01-03', '2015-12-30', '2015-12-31',\n                                                              '2016-01-02', '2016-01-03', '2016-12-30', '2016-12-31',\n                                                              '2017-01-02', '2017-01-03', '2017-12-30', '2017-12-31',\n                                                              '2018-01-02', '2018-01-03', '2018-12-30', '2018-12-31',\n                                                              '2019-01-02', '2019-01-03', '2019-12-30', '2019-12-31',\n                                                              '2015-07-02', '2015-07-03', '2015-07-05', '2015-07-06',\n                                                              '2016-07-02', '2016-07-03', '2016-07-05', '2016-07-06',\n                                                              '2017-07-02', '2017-07-03', '2017-07-05', '2017-07-06',\n                                                              '2018-07-02', '2018-07-03', '2018-07-05', '2018-07-06',\n                                                              '2019-07-02', '2019-07-03', '2019-07-05', '2019-07-06') THEN 'nearby_holiday' \"\"\"\n                                        \"ELSE 'non-holiday' END\"))\n\n  \n  # Feature 4: departure hour in local time\n  data = data.withColumn('local_departure_hour', hour(data.DEPT_DT))\n  '''\n  This dataset includes all original columns from the joined dataset as well as the engineered features. No extra encoding or transformation is included. \n  This dataset may contain missing values.\n  '''\n  data.write.mode('overwrite').parquet(f\"{blob_url}/full_dataset_full_features_v2\")  \n  return 'Full feature dataset is now generated'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64430c30-8f99-4f39-b2d7-5b3123da9f3f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def fill_missing(train,test):\n  '''\n  This function will fill missing values in training set for\n  1. numeric values: using median of the column (this operation will be achieved by pyspark.ml.feature.Imputer)\n  2. categorical values: adding \"NA\" value to make a new categorical value\n  \n Then the exactly same transformation will be applied to the test set.\n  \n  '''\n  categorical_cols = [\n    'QUARTER',\n    'MONTH',\n    'DAY_OF_MONTH',\n    'DAY_OF_WEEK',\n    'FL_DATE',\n    'OP_CARRIER',\n\n    'ORIGIN_STATE_ABR',\n    'DEST_STATE_ABR',\n\n    'WND_type_ORIGIN', 'WND_type_DEST',\n    'VIS_var_ORIGIN', 'VIS_var_DEST',          \n\n    'poten_for_del', \n\n    'holiday','local_departure_hour'                    \n\n                     ]\n  \n  numeric_cols = [\n  'WND_dir_ORIGIN', 'WND_dir_DEST',\n  'WND_spd_ORIGIN', 'WND_spd_DEST',\n  'VIS_dim_ORIGIN', 'VIS_dim_DEST',\n  'TMP_air_ORIGIN', 'TMP_air_DEST',    \n  'DEW_point_temp_ORIGIN', 'DEW_point_temp_DEST',\n  'SLP_pressure_ORIGIN', 'SLP_pressure_DEST',    \n  'CRS_ELAPSED_TIME',\n  'DISTANCE',\n  'avg_delay_ORIGIN', 'avg_delay_DEST'   \n    ]\n  # Impute missing numeric value by median of the column\n  imputer = Imputer(strategy='median',inputCols = numeric_cols,\n                   outputCols=[\"{}_imputed\".format(c) for c in numeric_cols]\n                   )\n  imputer_model = imputer.fit(train)\n \n  transformed_train = imputer_model.transform(train)\n  transformed_test = imputer_model.transform(test)\n  \n  # Replace the old columns\n  for c in numeric_cols:\n    transformed_train = transformed_train.drop(c)\n    transformed_train = transformed_train.withColumnRenamed(c+'_imputed', c)\n    transformed_test = transformed_test.drop(c)\n    transformed_test = transformed_test.withColumnRenamed(c+'_imputed', c)\n  \n  # Assign arbitrary value to missing categorical values\n  transformed_train = transformed_train.fillna(value='NA',subset = categorical_cols)\n  transformed_test = transformed_test.fillna(value='NA',subset = categorical_cols) \n  #prev_fl_del is binary, fillna with 0\n  transformed_train = transformed_train.fillna(value=0,subset = ['prev_fl_del'])\n  transformed_test = transformed_test.fillna(value=0,subset = ['prev_fl_del']) \n  return transformed_train,transformed_test"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"782381ef-d22f-4508-9889-85c95244ada3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def generate_ML_dataset():\n  '''\n  This function will first drop columns that are irrelevant to models, then fill missing values, and finally split it into train and test set.\n\n  '''  \n\n  t8_full = spark.read.parquet(f\"{blob_url}/full_dataset_full_features_v2\")\n  \n  # Keep relevant columns\n  t8_reduced = t8_full.select(\n\n  'YEAR',\n  'QUARTER',\n  'MONTH',\n  'DAY_OF_MONTH',\n  'DAY_OF_WEEK',\n  'FL_DATE',\n  'OP_CARRIER',\n\n  'ORIGIN_STATE_ABR',\n  'DEST_STATE_ABR',\n\n  'DEP_DEL15',\n  'CRS_ELAPSED_TIME',\n  'DISTANCE',\n\n  'WND_dir_ORIGIN', 'WND_dir_DEST',\n  'WND_spd_ORIGIN', 'WND_spd_DEST',\n  'VIS_dim_ORIGIN', 'VIS_dim_DEST',\n  'TMP_air_ORIGIN', 'TMP_air_DEST',\n\n  'DEW_point_temp_ORIGIN', 'DEW_point_temp_DEST',\n  'SLP_pressure_ORIGIN', 'SLP_pressure_DEST',\n\n  'WND_type_ORIGIN', 'WND_type_DEST',\n  'VIS_var_ORIGIN', 'VIS_var_DEST',  \n\n  'avg_delay_ORIGIN', 'avg_delay_DEST',  \n\n  'prev_fl_del', 'poten_for_del',\n\n  'holiday','local_departure_hour'\n\n  )\n  \n  # Drop rows where target variable is missing\n  t8_reduced = t8_reduced.dropna(subset=\"DEP_DEL15\")\n\n  # Make split into train/test set\n  t8_ML_train = t8_reduced.filter(t8_reduced['YEAR'] < 2019)\n  t8_ML_test = t8_reduced.filter(t8_reduced['YEAR'] == 2019)\n  \n  # Fill missing values\n  t8_ML_train_filled, t8_ML_test_filled = fill_missing(t8_ML_train, t8_ML_test)  \n  \n  # Save processed data\n  t8_ML_train_filled.write.mode(\"overwrite\").parquet(f\"{blob_url}/ML_train_filled\")\n  t8_ML_test_filled.write.mode(\"overwrite\").parquet(f\"{blob_url}/ML_test_filled\")\n  return 'train/test data have been created'\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6c84d08-2993-43d5-9acf-a0f7a5c02a27"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["if __name__ == \"__main__\":\n  # Preprocess the data and join tables\n  data_transformation()\n  # Apply feature engineering\n  feature_engineering()\n  # Split the data and apply the transformation (fill missing values)\n  generate_ML_dataset()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9642e693-bb8c-4996-bbf4-492818c35a25"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Full Data Pipeline","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":188115645378716}},"nbformat":4,"nbformat_minor":0}
