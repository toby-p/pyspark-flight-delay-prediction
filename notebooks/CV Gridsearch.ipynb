{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Set up data access:\n",
    "blob_container = \"w261team8rocks\"\n",
    "storage_account = \"dataguru\"\n",
    "secret_scope = \"w261-team8\"\n",
    "secret_key = \"cloudblob\"\n",
    "blob_url = f\"wasbs://w261team8rocks@dataguru.blob.core.windows.net\"\n",
    "mount_path = \"/mnt/mids-w261\"\n",
    "\n",
    "# SAS Token\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.sas.w261team8rocks.dataguru.blob.core.windows.net\",\n",
    "  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n",
    ")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "e8556949-2979-4fdc-8972-8b21bdb9fe62"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\"></div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "metadata": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "transient": null
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\nGridsearch with time-series cross-validation.\nAuthor: Toby Petty\nDate: 2021-11-21\n\"\"\"\n\nimport datetime\nimport os\nimport pandas as pd\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StandardScaler, StringIndexer, VectorAssembler\nfrom pyspark.sql.functions import to_timestamp, when\nimport random\nfrom sklearn.model_selection import ParameterGrid\nimport string\nimport time\n\n\nblob_url = f\"wasbs://w261team8rocks@dataguru.blob.core.windows.net\"\n\n\nclass SparkGridSearchTimeSeriesCV:\n    \"\"\"Custom class for time-series cross-validation with GridSearch.\"\"\"\n\n    def __init__(self, model_class, train, n_cv: int,\n                 numeric_features: list, categorical_features: list,\n                 one_hot_encode_features: list = None,\n                 grid_params: dict = None, static_params: dict = None,\n                 target: str = \"DEP_DEL15\", class_weights: bool = True,\n                 scale_features: bool = False, handle_invalid: str = \"error\"):\n        \"\"\"Time-series cross-validated, parameter gridsearch.\n\n        Args:\n            model_class: Spark estimator class.\n            train: full training dataframe.\n            n_cv: number of cross-validation splits. Each combination\n                of parameters will train n_cv-1 models.\n            numeric_features: list of numerical feature column names.\n            categorical_features: list of categorical feature column names.\n            one_hot_encode_features: subset of `categorical_features` which need\n                to be one-hot-encoded.\n            target: target feature column name.\n            grid_params: parameters to grid search.\n            static_params: additional model parameters which will be the\n                same in every instance of the estimator.\n            class_weights: whether or not to re-weight classes.\n            scale_features: whether to scale numeric features to mu=0, sigma=1.\n            handle_invalid: arg passed to Spark's feature classes. Can be\n                either `error` or `keep`.\n        \"\"\"\n        self.model_class = model_class\n        self.n_cv = n_cv\n        self.numeric_features = numeric_features\n        self.categorical_features = categorical_features\n        if one_hot_encode_features is None:\n            one_hot_encode_features = list()\n        self.one_hot_encode_features = one_hot_encode_features\n        assert set(self.one_hot_encode_features).issubset(set(self.categorical_features))\n        if grid_params is None:\n            grid_params = dict()\n        self.grid_params = grid_params\n        self.param_grid = ParameterGrid(self.grid_params)\n        if static_params is None:\n            static_params = dict()\n        self.static_params = static_params\n        self.target = target\n        self.class_weights = class_weights\n        if self.class_weights:\n            self.static_params[\"weightCol\"] = \"weight\"\n        self.scale_features = scale_features\n        self.hi = handle_invalid\n\n        # Add the target col and date col for cross-validation:\n        train = train.withColumn(\"target\", train[target])\n        train = train.withColumn(\"CV_DATE\", to_timestamp(train.FL_DATE, \"yyyy-MM-dd\"))\n        self.train = train\n\n        # Calculate date cut-offs for cross-validation:\n        dates_in_train = self.train.select(\"CV_DATE\").distinct().collect()\n        dates = sorted([row[\"CV_DATE\"] for row in dates_in_train])\n        n_dates = len(dates)\n        split_size = n_dates // n_cv\n        cv_splits = list()\n        train_start, train_end = 0, split_size\n        test_start, test_end = split_size, split_size * 2\n        for i in range(n_cv-1):\n            split_train = dates[train_start: train_end]\n            split_test = dates[test_start: test_end]\n            cv_splits.append((split_train, split_test))\n            train_end += split_size\n            test_start += split_size\n            test_end += split_size\n        self.cv_splits = cv_splits\n\n        # Dict to store all results:\n        self.results = dict()\n\n    def run(self):\n        # Generate unique random name for aggregating intermediate results:\n        self.run_uuid = \"\".join(random.choices(string.ascii_uppercase, k=6))  # NOQA.\n\n        for cv, (train_split, test_split) in enumerate(self.cv_splits, start=1):\n\n            print(f\"CV iteration {cv}:\")\n\n            self.results[cv] = dict()\n\n            # Get the subset of training data:\n            train_min_date, train_max_date = min(train_split), max(train_split)\n            train_subset = self.train.filter(\n                (self.train[\"CV_DATE\"] >= train_min_date) &\n                (self.train[\"CV_DATE\"] <= train_max_date)\n            ).cache()\n            print(f\"  Train: {train_min_date.strftime('%b %d %Y')} - {train_max_date.strftime('%b %d %Y')}\")\n            self.results[cv][\"train_min_date\"] = train_min_date\n            self.results[cv][\"train_max_date\"] = train_max_date\n\n            # Get the subset of test data:\n            test_min_date, test_max_date = min(test_split), max(test_split)\n            test_subset = self.train.filter(\n                (self.train[\"CV_DATE\"] >= test_min_date) &\n                (self.train[\"CV_DATE\"] <= test_max_date)\n            ).cache()\n            print(f\"  Test:  {test_min_date.strftime('%b %d %Y')} - {test_max_date.strftime('%b %d %Y')}\")\n            self.results[cv][\"test_min_date\"] = test_min_date\n            self.results[cv][\"test_max_date\"] = test_max_date\n\n            # Add weight column:\n            if self.class_weights:\n                class_weights = train_subset.groupBy(self.target).count().collect()\n                not_delayed = [r[\"count\"] for r in class_weights if r[self.target] == 0][0]\n                delayed = [r[\"count\"] for r in class_weights if r[self.target] == 1][0]\n                ratio = not_delayed / delayed\n                train_subset = train_subset.withColumn(\"weight\",\n                                                       when(train_subset[self.target] > 0, ratio).otherwise(1))\n\n            # Iterate through the parameter search space:\n            self.results[cv][\"models\"] = dict()\n            for i, parameters in enumerate(list(self.param_grid)):\n\n                t1 = time.time()\n\n                self.results[cv][\"models\"][i] = dict()\n                self.results[cv][\"models\"][i][\"parameters\"] = parameters\n                print(f\"    Model {i+1}: {parameters}\")\n\n                # Create the pipeline:\n                cat_features_ohe = list(set(self.categorical_features) & set(self.one_hot_encode_features))\n                cat_features = list(set(self.categorical_features) - set(self.one_hot_encode_features))\n\n                pipeline_stages, final_features = list(), list()\n\n                # One-Hot-Encoded categorical features:\n                if len(cat_features_ohe):\n                    ix_output_cols = [f\"{c}_ix\" for c in cat_features_ohe]\n                    ohe_ix = StringIndexer(inputCols=cat_features_ohe, outputCols=ix_output_cols, handleInvalid=self.hi)\n                    ohe_output_cols = [f\"{c}_ohe\" for c in cat_features_ohe]\n                    ohe = OneHotEncoder(inputCols=ix_output_cols, outputCols=ohe_output_cols, handleInvalid=self.hi)\n                    pipeline_stages += [ohe_ix, ohe]\n                    final_features += ohe_output_cols\n\n                # Other categorical features, not One-Hot-Encoded:\n                if len(cat_features):\n                    cat_output_cols = [f\"{c}_ix\" for c in cat_features]\n                    cat_ix = StringIndexer(inputCols=cat_features, outputCols=cat_output_cols, handleInvalid=self.hi)\n                    pipeline_stages.append(cat_ix)\n                    final_features += cat_output_cols\n\n                # Numeric features:\n                if len(self.numeric_features):\n                    num_va = VectorAssembler(inputCols=self.numeric_features, outputCol=\"num_features\",\n                                             handleInvalid=self.hi)\n                    pipeline_stages.append(num_va)\n                    if self.scale_features:  # Apply standard scaling:\n                        scaler = StandardScaler(inputCol=\"num_features\", outputCol=\"scaled_num_features\",\n                                                withMean=True, withStd=True)\n                        pipeline_stages.append(scaler)\n                        final_features.append(\"scaled_num_features\")\n                    else:\n                        final_features.append(\"num_features\")\n\n                # Create final features:\n                self.results[cv][\"models\"][i][\"final_features\"] = final_features\n                feature_assembler = VectorAssembler(inputCols=final_features, outputCol=\"features\")\n                pipeline_stages.append(feature_assembler)\n\n                # Create the model instance:\n                model = self.model_class(labelCol=\"target\", featuresCol=\"features\", **parameters, **self.static_params)\n                self.results[cv][\"models\"][i][\"model\"] = model\n\n                # Create the full pipeline:\n                pipeline = Pipeline(stages=pipeline_stages + [model]).fit(train_subset)\n\n                # Make the test set predictions:\n                test_pred = pipeline.transform(test_subset)\n\n                # Compute the confusion matrix:\n                spark_cm = test_pred.groupBy(\"prediction\").pivot(\"target\").count().collect()\n                pd_cm = pd.DataFrame([r.asDict() for r in spark_cm]).set_index(\"prediction\")\n                pd_cm.columns.name = \"target\"\n                self.results[cv][\"models\"][i][\"conf_matrix\"] = pd_cm\n\n                # Calculate time taken:\n                self.results[cv][\"models\"][i][\"training_time\"] = time.time() - t1\n\n                # Save the intermediate results in case the program crashes later:\n                self.save_results(verbose=False)\n\n            train_subset.unpersist()\n            test_subset.unpersist()\n\n    @property\n    def results_df(self):\n        \"\"\"Combine all results into easy to read pandas.DataFrame.\"\"\"\n        rows = list()\n        for k1, v1 in self.results.items():\n            for k2, v2 in v1[\"models\"].items():\n                conf_matrix = v2[\"conf_matrix\"].reindex([0, 1]).fillna(0).astype(int)\n                row = {\n                    \"n_cv\": self.n_cv,\n                    \"cv_iter\": k1,\n                    \"model_num\": k2,\n                    \"training_time\": v2[\"training_time\"],\n                    \"train_min_date\": v1[\"train_min_date\"],\n                    \"train_max_date\": v1[\"train_max_date\"],\n                    \"test_min_date\": v1[\"test_min_date\"],\n                    \"test_max_date\": v1[\"test_max_date\"],\n                    \"parameters\": v2[\"parameters\"],\n                    \"static_params\": self.static_params,\n                    \"numeric_features\": self.numeric_features,\n                    \"categorical_features\": self.categorical_features,\n                    \"one_hot_encode_features\": self.one_hot_encode_features,\n                    \"tp\": conf_matrix.loc[1][1],\n                    \"fp\": conf_matrix.loc[1][0],\n                    \"tn\": conf_matrix.loc[0][0],\n                    \"fn\": conf_matrix.loc[0][1],\n                }\n                rows.append(row)\n        df = pd.DataFrame(rows)\n        df[\"model\"] = str(self.model_class)\n        df[\"target\"] = self.target\n        df[\"timestamp\"] = datetime.datetime.now()  # Timestamp for aggregating results.\n        df[\"class_weights\"] = self.class_weights\n        df[\"scale_features\"] = self.scale_features\n        df[\"handle_invalid\"] = self.hi\n        column_order = [\"model\"] + [c for c in df.columns if c != \"model\"]\n        return df[column_order]\n\n    def save_results(self, verbose: bool = True):\n        name = \"\".join(list(filter(lambda s: s not in \"<>'\", str(self.model_class)))).replace(\".\", \"_\")\n        now = datetime.datetime.now().strftime(\"%Y-%m-%d %H_%M_%S\")\n        filename = f\"{self.run_uuid} - {name} - {now}\"\n        df = self.results_df\n\n        # Save CSV:\n        filepath = os.path.join(\"/dbfs/team8_results\", f\"{filename}.csv\")\n        df.to_csv(filepath, encoding=\"utf-8\", index=False)\n        if verbose:\n            print(f\"Results CSV saved to: {filepath}\")\n\n        # Save to blob storage:\n        filepath = f\"{blob_url}/{filename}\"\n        spark_df = spark.createDataFrame(df.astype(str))  # NOQA.\n        spark_df.write.mode(\"overwrite\").parquet(filepath)\n        if verbose:\n            print(f\"Results saved to blob storage: {filepath}\")\n"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "768a24d6-c61d-4970-8b25-54e17ecbe72b"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\"></div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "metadata": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "transient": null
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the training dataset:\ndata = spark.read.parquet(f\"{blob_url}/ML_train_filled\")\ndata = data.dropna(subset=\"DEP_DEL15\")\ntrain = data.filter(data[\"YEAR\"] < 2019).cache()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "dc3691cf-bfe0-4d89-ac0c-a8b78dfaa51c"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\"></div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "metadata": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "transient": null
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GBTClassifier"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "c0dceb40-df9b-4304-a8c0-d84d5f7642e8"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n\n# Define the feature set:\n\n# Outcome variable - 1/0 binary variable:\ntarget = \"DEP_DEL15\"\n\n# Features to use in the model:\nnumeric_features = [\n    \"WND_dir_ORIGIN\",  # Wind direction at origin.\n    \"WND_dir_DEST\",  # Wind direction at destination.\n    \"WND_spd_ORIGIN\",  # Wind speed at origin.\n    \"WND_spd_DEST\",  # Wind speed at destination.\n    \"VIS_dim_ORIGIN\",  # Visibility at origin.\n    \"VIS_dim_DEST\",  # Visibility at destination.\n    \"TMP_air_ORIGIN\",  # Air temperature at origin.\n    \"TMP_air_DEST\",  # Air temperature at destination.\n    \"DEW_point_temp_ORIGIN\",  # Dew point temperature at origin.\n    \"DEW_point_temp_DEST\",  # Dew point temperature at destination.\n    \"SLP_pressure_ORIGIN\",  # Sea-level pressure at origin.\n    \"SLP_pressure_DEST\",  # Sea-level pressure at destination.\n    \"CRS_ELAPSED_TIME\",  # Scheduled flight time.\n    \"DISTANCE\",  # Scheduled flight distance.\n    \"avg_delay_ORIGIN\",  # Rolling average delay at origin.\n    \"avg_delay_DEST\",  # Rolling average delay at destination.\n]\n\ncategorical_features = [\n    \"MONTH\",  # Month of year flight departed.\n    \"DAY_OF_WEEK\",  # Day of week flight departed.\n    \"OP_CARRIER\",  # Airline.\n    \"ORIGIN_STATE_ABR\",  # Origin state.\n    \"DEST_STATE_ABR\",  # Destination state.\n    \"WND_type_ORIGIN\",  # Classification of wind at origin.\n    \"WND_type_DEST\",  # Classification of wind at destination.\n    \"VIS_var_ORIGIN\",  # Clasification of visibility at origin.\n    \"VIS_var_DEST\",  # Clasification of visibility at destination.\n    \"prev_fl_del\",  # Was the aircraft's previous flight delayed?\n    \"poten_for_del\",  # Indicator feature based on whether aircraft is at the airport.\n    \"holiday\",  # Was departure date a holiday?\n    \"local_departure_hour\",  # Hour of flight departure.\n]\n\none_hot_encode_features = None\n\n# Parameters to gridsearch:\ngrid_params = {\n    \"maxIter\": [25, 75],  # Number of gradient boosting iterations.\n    \"minInstancesPerNode\": [1, 5],  # Minimum number of instances each leaf node must have.\n    \"maxDepth\": [1, 2, 4],  # Maximum depth of each tree (max allowed is 30).\n}\n\n# Parameters to use in every model version:\nstatic_params = {\n    \"maxBins\": 100,  # Must be more than 52 - number of categories in the state features.\n    \"stepSize\": 0.1,  # Learning rate.\n}\n\ngscv = SparkGridSearchTimeSeriesCV(\n    model_class = GBTClassifier,  # The estimator class.\n    train = train,  # The full training dataset.\n    n_cv = 4,  # The number of cross-validation splits (will train n_cv-1 models).\n    numeric_features = numeric_features,  # Column names of numeric features to use.\n    categorical_features = categorical_features,  # Column names of categorical features to use.\n    one_hot_encode_features = one_hot_encode_features,  # Column names of categorical features to also one-hot-encode.\n    grid_params = grid_params,  # Dict of parameter space to be searched.\n    static_params = static_params,  # Additional static parameters to use in every version of the model.\n    target = target,  # Name of column being predicted.\n    class_weights = True,  # Whether to reweight class imbalance.\n    scale_features = False,  # Whether to scale numeric features to mu=0, sigma=1.\n    handle_invalid = \"keep\",  # What to do with Spark feature class errors.\n)\n\n# Train the models:\ngscv.run()\n\n# Save the full results to CSV:\ngscv.save_results()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "69f83bdb-1106-4201-aef4-096b153e26c8"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\"></div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "metadata": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "transient": null
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RandomForestClassifier"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "34218f5e-0c3d-475d-8715-ab62f39e17bf"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n\n# Features to use in the model:\nnumeric_features = ['WND_dir_ORIGIN', 'WND_dir_DEST', # Wind direction (Weather)\n                    'WND_spd_ORIGIN', 'WND_spd_DEST', # Wind speed (Weather)\n                    'VIS_dim_ORIGIN', 'VIS_dim_DEST', # Visibility distance (Weather)\n                    'TMP_air_ORIGIN', 'TMP_air_DEST', # Air temperature (Weather)\n                    'DEW_point_temp_ORIGIN', 'DEW_point_temp_DEST', # Dew point temperature (Weather)\n                    'SLP_pressure_ORIGIN', 'SLP_pressure_DEST', # Sea level pressure (Weather)\n                    'CRS_ELAPSED_TIME', # Scheduled airtime (Flight)\n                    'avg_delay_ORIGIN', 'avg_delay_DEST', # Average delay (minutes) 2-6 hours (New feature)\n                    'DISTANCE' # Distance between origin and destination\n]\ncategorical_features = ['WND_type_ORIGIN', 'WND_type_DEST', # Wind type (Weather)\n                        'VIS_var_ORIGIN', 'VIS_var_DEST', # Visibility variability (Weather)\n                        'OP_CARRIER', # Airplane carrier (Flight)\n                        'QUARTER', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', # Day/time columns (Flight)\n                        'ORIGIN_STATE_ABR', 'DEST_STATE_ABR', # State abbreviation (Flight)\n                        'prev_fl_del', 'poten_for_del', # Prior flight delay & potential for delay (New features)\n                        'holiday', # Holiday indicator (New feature)\n                        'local_departure_hour' # Departure hour local (Flight)\n                        #'OD_pair' # Origin-destination Airport pairs (Flight)\n]\n\n# Parameters to gridsearch:\ngrid_params = {\n    'numTrees': [20, 40],\n    'maxDepth': [3, 5, 7]\n}\n\n# Parameters to use in every model version:\nstatic_params = {\n    \"weightCol\": \"weight\",\n    \"maxBins\":100\n}\n                        \n# Create the instance:\ngscv = SparkGridSearchTimeSeriesCV(\n    model_class = RandomForestClassifier,  # The estimator class.\n    train = train,  # The full training dataset.\n    n_cv = 4,  # The number of cross-validation splits (will train n_cv-1 models).\n    numeric_features = numeric_features,  # Column names of categorical features to use.\n    categorical_features = categorical_features,  # Column names of numeric features to use.\n    grid_params = grid_params,  # Dict of parameter space to be searched.\n    static_params = static_params,  # Additional static parameters to use in every version of the model.\n    target = \"DEP_DEL15\",  # Name of column being predicted.\n    class_weights = True,  # Whether to reweight class imbalance.\n    scale_features = False,  # Whether to scale numeric features to mu=0, sigma=1.\n    handle_invalid = \"keep\",  # What to do with Spark feature class errors.\n)\n\n# Train the models:\ngscv.run()\n\n# Save the results to CSV:\ngscv.save_results()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "ad902b68-be5d-43da-8e73-9eaaba7111b1"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\"></div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "metadata": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "transient": null
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LogisticRegression"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "aa49fd27-4bc1-4d48-87af-1871e1ffa8f3"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n\n# Features to use in the model:\nnumeric_features = ['WND_dir_ORIGIN', 'WND_dir_DEST', # Wind direction (Weather)\n                    'WND_spd_ORIGIN', 'WND_spd_DEST', # Wind speed (Weather)\n                    'VIS_dim_ORIGIN', 'VIS_dim_DEST', # Visibility distance (Weather)\n                    'TMP_air_ORIGIN', 'TMP_air_DEST', # Air temperature (Weather)\n                    'DEW_point_temp_ORIGIN', 'DEW_point_temp_DEST', # Dew point temperature (Weather)\n                    'SLP_pressure_ORIGIN', 'SLP_pressure_DEST', # Sea level pressure (Weather)\n                    'CRS_ELAPSED_TIME', # Scheduled airtime (Flight)\n                    'avg_delay_ORIGIN', 'avg_delay_DEST', # Average delay (minutes) 2-6 hours (New feature)\n                    'DISTANCE' # Distance between origin and destination\n]\ncategorical_features = ['WND_type_ORIGIN', 'WND_type_DEST', # Wind type (Weather)\n                        'VIS_var_ORIGIN', 'VIS_var_DEST', # Visibility variability (Weather)\n                        'OP_CARRIER', # Airplane carrier (Flight)\n                        'QUARTER', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', # Day/time columns (Flight)\n                        'ORIGIN_STATE_ABR', 'DEST_STATE_ABR', # State abbreviation (Flight)\n                        'prev_fl_del', 'poten_for_del', # Prior flight delay & potential for delay (New features)\n                        'holiday', # Holiday indicator (New feature)\n                        'local_departure_hour' # Departure hour local (Flight)\n                        #'OD_pair' # Origin-destination Airport pairs (Flight)\n]\n\n# Parameters to gridsearch:\ngrid_params = {\n    'regParam': [0, 0.01], # 0 means no regularization\n    'elasticNetParam': [0, 0.001, 0.01, 0.1, 0.5, 1], # Elastic net contains both L1 and L2 regularization. Elastic net parameter α set to 1 = Lasso model; If α is set to 0 = ridge regression model. L1 removes coefficient (goes to zero). L1 regularization technique is called Lasso Regression (α set to 1) and model which uses L2 is called Ridge Regression (α set to 0).\n}\n\n# Parameters to use in every model version:\nstatic_params = {\n    \"weightCol\": \"weight\", # turn on class_weights\n    \"maxIter\": 10\n}\n                        \n# Create the instance:\ngscv = SparkGridSearchTimeSeriesCV(\n    model_class = LogisticRegression,  # The estimator class.\n    train = train,  # The full training dataset.\n    n_cv = 4,  # The number of cross-validation splits (will train n_cv-1 models).\n    numeric_features = numeric_features,  # Column names of categorical features to use.\n    categorical_features = categorical_features,  # Column names of numeric features to use.\n    one_hot_encode_features = categorical_features, # Logistic Regression - turn all categorical features to one hot encode features\n    grid_params = grid_params,  # Dict of parameter space to be searched.\n    static_params = static_params,  # Additional static parameters to use in every version of the model.\n    target = \"DEP_DEL15\",  # Name of column being predicted.\n    class_weights = True,  # Whether to reweight class imbalance.\n    scale_features = True,  # Whether to scale numeric features to mu=0, sigma=1.\n)\n\n# Train the models:\ngscv.run()\n\n# Save the results to CSV:\ngscv.save_results()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "04d0a221-e957-4c7a-8df8-eb167375df59"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\"></div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "metadata": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "transient": null
    }
   ],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "CV Gridsearch",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 1089197539916944
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}